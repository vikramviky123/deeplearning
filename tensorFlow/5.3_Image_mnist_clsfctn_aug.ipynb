{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import  Path\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Dropout\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, Resizing, Rescaling\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives\n",
    "from tensorflow.keras.metrics import TrueNegatives, Precision, Recall, AUC, binary_accuracy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers  import L2, L1\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_set = tf.data.Dataset.range(10)\n",
    "print(list(d_set.as_numpy_iterator()))\n",
    "\n",
    "ll = len(list(d_set.as_numpy_iterator()))\n",
    "\n",
    "tr_d_set = d_set.take(int(ll*0.6))\n",
    "print(list(tr_d_set.as_numpy_iterator()))\n",
    "\n",
    "val_d_set = d_set.skip(int(ll*0.6))\n",
    "val_set = val_d_set.take(int(ll*0.2))\n",
    "print(list(val_set.as_numpy_iterator()))\n",
    "\n",
    "ts_d_set = val_d_set.skip(int(ll*0.2))\n",
    "print(list(ts_d_set.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = splits(ds_train, TRAIN_RATIO, VAL_RATIO)\n",
    "# print(list(train_dataset.take(1).as_numpy_iterator()),\n",
    "#     list(val_dataset.take(1).as_numpy_iterator()), list(test_dataset.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for i, (image, label) in enumerate(train_dataset.take(16)):\n",
    "  ax = plt.subplot(4, 8, i + 1)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  plt.title(ds_info.features['label'].int2str(label.numpy()))\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"N_EPOCHS\": 5,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"DROPOUT_RATE\": 0.0,\n",
    "    \"IM_SIZE\": 24,\n",
    "    \"REGULARIZATION_RATE\": 0.0,\n",
    "    \"N_FILTERS\": 6,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 1,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_DENSE_1\": 100,\n",
    "    \"N_DENSE_2\": 10,\n",
    "}\n",
    "IM_SIZE =24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_rescale(image, label):\n",
    "    return tf.image.resize(image, (IM_SIZE, IM_SIZE)) / 255., label\n",
    "\n",
    "\n",
    "def augment_0(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_1(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  image = tf.image.random_brightness(image, 0.2)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def augment_2(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  image = tf.image.random_flip_up_down(image)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def augment_3(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  image = tf.image.flip_left_right(image)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def augment_4(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  image = tf.image.rot90(image)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def augment_5(image, label):\n",
    "  image, label = resize_rescale(image, label)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(original_image, Augmented_image):\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(original_image)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(Augmented_image)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image, label = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmented_image = tf.image.rot90(original_image, k=1, name=None)\n",
    "Augmented_image = tf.image.flip_left_right(original_image)\n",
    "Augmented_image = tf.image.adjust_brightness(original_image, delta=0.8)\n",
    "Augmented_image = tf.image.adjust_contrast(original_image, contrast_factor=19)\n",
    "Augmented_image = tf.image.random_flip_up_down(original_image)\n",
    "Augmented_image = tf.image.rot90(original_image, k=2)\n",
    "\n",
    "# Convert grayscale images to RGB\n",
    "rgb_images = tf.image.grayscale_to_rgb(original_image)\n",
    "Augmented_image = tf.image.random_saturation(rgb_images, lower=2, upper=12)\n",
    "Augmented_image = tf.image.adjust_saturation(\n",
    "    rgb_images, saturation_factor=0.9, name=None)\n",
    "Augmented_image = tf.image.random_brightness(\n",
    "    rgb_images, max_delta=0.1, seed=None)\n",
    "Augmented_image = tf.image.adjust_gamma(original_image, gamma=0.1, gain=0.1)\n",
    "\n",
    "\n",
    "Augmented_image = tf.image.central_crop(original_image, central_fraction=0.5)\n",
    "\n",
    "visualize(original_image, Augmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_0 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_0)\n",
    "    )\n",
    "val_dataset_0 = (\n",
    "    val_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_0)\n",
    "    )\n",
    "train_dataset_1 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_1)\n",
    "    )\n",
    "val_dataset_1 = (\n",
    "    val_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_1)\n",
    "    )\n",
    "train_dataset_2 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_2)\n",
    "    )\n",
    "val_dataset_2 = (\n",
    "    val_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .map(augment_2)\n",
    ")\n",
    "train_dataset_3 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_3)\n",
    "    )\n",
    "val_dataset_3 = (\n",
    "    val_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .map(augment_3)\n",
    ")\n",
    "train_dataset_4 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_4)\n",
    "    )\n",
    "val_dataset_4 = (\n",
    "    val_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .map(augment_4)\n",
    ")\n",
    "train_dataset_5 = (\n",
    "        train_dataset\n",
    "        .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "        .map(augment_5)\n",
    "    )\n",
    "val_dataset_5 = (\n",
    "    val_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .map(augment_5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = train_dataset_0.concatenate(train_dataset_1).concatenate(train_dataset_2).concatenate(\n",
    "    train_dataset_3).concatenate(train_dataset_4).concatenate(train_dataset_5)\n",
    "full_val_dataset = val_dataset_0.concatenate(val_dataset_1).concatenate(val_dataset_2).concatenate(\n",
    "    val_dataset_3).concatenate(val_dataset_4).concatenate(val_dataset_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = (\n",
    "    full_train_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(128)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "full_val_data = (\n",
    "    full_val_dataset\n",
    "    .shuffle(buffer_size=1024, reshuffle_each_iteration=True)\n",
    "    .batch(128)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = ds_test.map(resize_rescale)\n",
    "# test_data = test_.shuffle(\n",
    "#     buffer_size=8, reshuffle_each_iteration=True).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, lab = next(iter(full_train_data.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in full_train_data.take(1):\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Layer):\n",
    "    def __init__(self,filters,kernel_size,strides,padding,activation,pool_size):\n",
    "        super(FeatureExtractor,self).__init__()\n",
    "        \n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation)\n",
    "        self.batch_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.maxpool_1 = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=strides*2, padding=padding)\n",
    "        \n",
    "        self.conv_2 = tf.keras.layers.Conv2D(filters=filters*2+4, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation)\n",
    "        self.batch_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.maxpool_2 = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=strides*2, padding=padding)\n",
    "        \n",
    "    def call(self,x, training):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        x = self.maxpool_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class ModelExtractor(Model):\n",
    "    def __init__(self):\n",
    "        super(ModelExtractor, self).__init__()\n",
    "\n",
    "        self.fetaure_extractor = FeatureExtractor(filters=6,\n",
    "                                       kernel_size=5,\n",
    "                                       strides=1,\n",
    "                                       padding='valid',\n",
    "                                       activation='relu',\n",
    "                                       pool_size=2)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_1 = tf.keras.layers.Dense(120, activation='relu')\n",
    "        self.batch_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense_2 = tf.keras.layers.Dense(84, activation='relu')\n",
    "        self.batch_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dense_3 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.fetaure_extractor(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.dense_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        \n",
    "        x = self.dense_2(x)\n",
    "        x = self.batch_2(x)\n",
    "\n",
    "        x = self.dense_3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "feature_sub_classed = FeatureExtractor(filters=6,\n",
    "                                       kernel_size=5,\n",
    "                                       strides=1,\n",
    "                                       padding='valid',\n",
    "                                       activation='relu',\n",
    "                                       pool_size=2)\n",
    "\n",
    "model_sub_classed = ModelExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_input = Input(shape=(24, 24, 1), name=\"Input Image\")\n",
    "\n",
    "func_output = model_sub_classed(func_input)\n",
    "\n",
    "lenet_model_sub_classed = Model(func_input, func_output, name=\"Lenet_Model_sub_classed\")\n",
    "\n",
    "lenet_model_sub_classed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if (logs.get('val_sparse_categorical_accuracy')>=0.992):\n",
    "            print(\"99.5% Accuracy is reached .......Hence Stopping\")\n",
    "            self.model.stop_training=True  \n",
    "                                                                                   \n",
    "# Giving call back for accuracy\n",
    "callbacks = CustomCallback() \n",
    "\n",
    "# Giving call back for EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "      \n",
    "# Logging the losses and metrics data\n",
    "filename = 'log.csv'\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "\n",
    "# Changing Learning Rate with a Scheduler function\n",
    "def schedule_func(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule_func)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "                                                    factor=0.1,\n",
    "                                                    patience=2,\n",
    "                                                    verbose=1,\n",
    "                                                    mode='auto',\n",
    "                                                    min_delta=0.0001,\n",
    "                                                    min_lr=0.0000001\n",
    "                                                )\n",
    "\n",
    "# Saving the best model and its weights to given path\n",
    "checkpoint_filepath = 'weights/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                                    filepath=checkpoint_filepath,\n",
    "                                                                    verbose = 1,\n",
    "                                                                    save_weights_only=True,\n",
    "                                                                    save_best_only=True,\n",
    "                                                                    monitor='val_loss',\n",
    "                                                                    mode='auto'\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model_sub_classed.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "lenet_history = lenet_model_sub_classed.fit(full_train_data,\n",
    "                                            validation_data=full_val_data,\n",
    "                                            epochs=50,\n",
    "                                            verbose=2,\n",
    "                                            callbacks = [callbacks, \n",
    "                                                        early_stopping, \n",
    "                                                        csv_logger, \n",
    "                                                        lr_scheduler, \n",
    "                                                        model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(lenet_history.history['loss'], label=\"train_loss\")\n",
    "ax[0].plot(lenet_history.history['val_loss'], label=\"val_loss\")\n",
    "ax[0].set_title(\"SCE loss function\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(lenet_history.history['sparse_categorical_accuracy'],\n",
    "           label=\"train_acc\")\n",
    "ax[1].plot(lenet_history.history['val_sparse_categorical_accuracy'],\n",
    "           label=\"val_acc\")\n",
    "ax[1].set_title(\"Accuracy Metric function\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model_sub_classed.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "for i, (image, label) in enumerate(test_data.take(16)):\n",
    "\n",
    "  ax = plt.subplot(4, 8, i + 1)\n",
    "\n",
    "  plt.imshow(tf.squeeze(image, axis=0))\n",
    "\n",
    "  pred_prob_lst = lenet_model_sub_classed.predict(image)\n",
    "\n",
    "  pred_label = np.argmax(pred_prob_lst)\n",
    "\n",
    "  pred_prob = np.max(pred_prob_lst)\n",
    "\n",
    "  plt.title(f\"label:{pred_label} prob:{pred_prob:.2f}\")\n",
    "  plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model_sub_classed.save_weights('weights/lenet_weights')\n",
    "lenet_model_sub_classed.load_weights('weights/lenet_weights')\n",
    "lenet_model_sub_classed.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model_sub_classed.load_weights(checkpoint_filepath)\n",
    "lenet_model_sub_classed.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
